{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0095e083",
   "metadata": {},
   "source": [
    "# Creating the model that will be used for classifying financial sentences on Streamlit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077bacf",
   "metadata": {},
   "source": [
    "## Downloading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108f5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from numpy import linspace\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import pysentiment2 as ps\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "#from keras.optimizers import SGD doesnt work\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import keras.optimizers\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#from keras.optimizers import RMSprop doesnt work\n",
    "#from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm  # better for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1ed9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18425, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/xeb15154/OneDrive - University of Strathclyde/PhD/Excel Files (Data)')\n",
    "df = pd.read_csv('new_classified_audio_test1(verified).csv')\n",
    "del df['Unnamed: 0.1']\n",
    "del df['Unnamed: 0']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6840e4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified = df[df['sentiment'].notnull()]\n",
    "df_classified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0775946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_manual\n",
       " 0.0    1469\n",
       " 1.0     304\n",
       "-1.0     111\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified['Sentiment_manual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b3bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the sampled df\n",
    "os.chdir('H:/')\n",
    "sampled_df = pd.read_csv('sampled_df_equalweighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "686a95f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>new</th>\n",
       "      <th>speech</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiment_manual</th>\n",
       "      <th>name</th>\n",
       "      <th>session</th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>quarter</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer local</th>\n",
       "      <th>Shimmer local dB</th>\n",
       "      <th>Shimmer apq3</th>\n",
       "      <th>Shimmer apq5</th>\n",
       "      <th>Shimmer apq11</th>\n",
       "      <th>Shimmer dda</th>\n",
       "      <th>Mean autocorrelation</th>\n",
       "      <th>Mean NHR</th>\n",
       "      <th>Mean HNR</th>\n",
       "      <th>Audio Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>JPM_57713_000645</td>\n",
       "      <td>And if you think about 11%plus tier one capita...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marianne Lake</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>11/10/2013 16:36</td>\n",
       "      <td>JPM_57713</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.130</td>\n",
       "      <td>1.138</td>\n",
       "      <td>4.807</td>\n",
       "      <td>7.172</td>\n",
       "      <td>12.167</td>\n",
       "      <td>14.422</td>\n",
       "      <td>0.862177</td>\n",
       "      <td>0.194742</td>\n",
       "      <td>10.197</td>\n",
       "      <td>17.513991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CVX_181025_000460</td>\n",
       "      <td>So there is no primary need on balance sheet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pierre Breber</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>29/01/2021 23:14</td>\n",
       "      <td>CVX_181025</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14.836</td>\n",
       "      <td>1.244</td>\n",
       "      <td>6.570</td>\n",
       "      <td>9.223</td>\n",
       "      <td>13.112</td>\n",
       "      <td>19.710</td>\n",
       "      <td>0.786955</td>\n",
       "      <td>0.311092</td>\n",
       "      <td>6.546</td>\n",
       "      <td>7.341958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LMT_171182_000325</td>\n",
       "      <td>If theyre not sure theyre going to get paid fo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James Taiclet</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>21/07/2020 21:21</td>\n",
       "      <td>LMT_171182</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.822</td>\n",
       "      <td>1.228</td>\n",
       "      <td>5.390</td>\n",
       "      <td>7.708</td>\n",
       "      <td>13.904</td>\n",
       "      <td>16.169</td>\n",
       "      <td>0.834549</td>\n",
       "      <td>0.244605</td>\n",
       "      <td>8.601</td>\n",
       "      <td>40.501746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AXP_22181_000347</td>\n",
       "      <td>You may have seen that we were rated 1 in cust...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Daniel T. Henry</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>23/10/2009 04:51</td>\n",
       "      <td>AXP_22181</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>14.428</td>\n",
       "      <td>1.260</td>\n",
       "      <td>6.723</td>\n",
       "      <td>9.102</td>\n",
       "      <td>12.561</td>\n",
       "      <td>20.169</td>\n",
       "      <td>0.840758</td>\n",
       "      <td>0.206720</td>\n",
       "      <td>7.759</td>\n",
       "      <td>24.655147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>UNP_149565_000810</td>\n",
       "      <td>That’s all from me</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cherilyn Radbourne</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>18/04/2019 21:18</td>\n",
       "      <td>UNP_149565</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17.217</td>\n",
       "      <td>1.483</td>\n",
       "      <td>7.715</td>\n",
       "      <td>10.271</td>\n",
       "      <td>18.200</td>\n",
       "      <td>23.144</td>\n",
       "      <td>0.751303</td>\n",
       "      <td>0.386504</td>\n",
       "      <td>5.624</td>\n",
       "      <td>5.481270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>2101</td>\n",
       "      <td>HON_86730_000093</td>\n",
       "      <td>What was the process?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Scott Davis</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>28/07/2015 19:02</td>\n",
       "      <td>HON_86730</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>21.467</td>\n",
       "      <td>1.755</td>\n",
       "      <td>10.289</td>\n",
       "      <td>16.527</td>\n",
       "      <td>11.007</td>\n",
       "      <td>30.866</td>\n",
       "      <td>0.788477</td>\n",
       "      <td>0.300832</td>\n",
       "      <td>6.411</td>\n",
       "      <td>58.961875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>2102</td>\n",
       "      <td>BLK_77575_000300</td>\n",
       "      <td>But I do believe in this divergent world and t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Larry Fink</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>16/01/2015 04:09</td>\n",
       "      <td>BLK_77575</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>21.663</td>\n",
       "      <td>1.711</td>\n",
       "      <td>10.139</td>\n",
       "      <td>18.930</td>\n",
       "      <td>27.440</td>\n",
       "      <td>30.416</td>\n",
       "      <td>0.741760</td>\n",
       "      <td>0.402328</td>\n",
       "      <td>5.312</td>\n",
       "      <td>2.973515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>2103</td>\n",
       "      <td>LLY_28408_000376</td>\n",
       "      <td>I wan to remind the investment community once ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Derica Rice</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>21/10/2010 20:35</td>\n",
       "      <td>LLY_28408</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>19.008</td>\n",
       "      <td>1.620</td>\n",
       "      <td>9.075</td>\n",
       "      <td>12.276</td>\n",
       "      <td>18.522</td>\n",
       "      <td>27.224</td>\n",
       "      <td>0.770155</td>\n",
       "      <td>0.347879</td>\n",
       "      <td>6.351</td>\n",
       "      <td>8.772698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>2104</td>\n",
       "      <td>CVX_37942_000410</td>\n",
       "      <td>That was  the quarter recognized 2 of those dr...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patricia E. Yarrington</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>27/04/2012 19:51</td>\n",
       "      <td>CVX_37942</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12.630</td>\n",
       "      <td>1.035</td>\n",
       "      <td>5.229</td>\n",
       "      <td>6.777</td>\n",
       "      <td>8.244</td>\n",
       "      <td>15.688</td>\n",
       "      <td>0.871881</td>\n",
       "      <td>0.174371</td>\n",
       "      <td>10.346</td>\n",
       "      <td>36.462494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>2105</td>\n",
       "      <td>PLD_171179_000349</td>\n",
       "      <td>On cap rate compression, we are already seeing...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamid Moghadam</td>\n",
       "      <td>question_answer</td>\n",
       "      <td>21/07/2020 21:03</td>\n",
       "      <td>PLD_171179</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.721</td>\n",
       "      <td>1.269</td>\n",
       "      <td>6.435</td>\n",
       "      <td>9.067</td>\n",
       "      <td>12.255</td>\n",
       "      <td>19.306</td>\n",
       "      <td>0.689297</td>\n",
       "      <td>0.484395</td>\n",
       "      <td>3.661</td>\n",
       "      <td>3.549958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2106 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                new  \\\n",
       "0              0   JPM_57713_000645   \n",
       "1              1  CVX_181025_000460   \n",
       "2              2  LMT_171182_000325   \n",
       "3              3   AXP_22181_000347   \n",
       "4              4  UNP_149565_000810   \n",
       "...          ...                ...   \n",
       "2101        2101   HON_86730_000093   \n",
       "2102        2102   BLK_77575_000300   \n",
       "2103        2103   LLY_28408_000376   \n",
       "2104        2104   CVX_37942_000410   \n",
       "2105        2105  PLD_171179_000349   \n",
       "\n",
       "                                                 speech  sentiment  \\\n",
       "0     And if you think about 11%plus tier one capita...        1.0   \n",
       "1          So there is no primary need on balance sheet        0.0   \n",
       "2     If theyre not sure theyre going to get paid fo...       -1.0   \n",
       "3     You may have seen that we were rated 1 in cust...        1.0   \n",
       "4                                    That’s all from me        0.0   \n",
       "...                                                 ...        ...   \n",
       "2101                              What was the process?        0.0   \n",
       "2102  But I do believe in this divergent world and t...        1.0   \n",
       "2103  I wan to remind the investment community once ...       -1.0   \n",
       "2104  That was  the quarter recognized 2 of those dr...       -1.0   \n",
       "2105  On cap rate compression, we are already seeing...       -1.0   \n",
       "\n",
       "      Sentiment_manual                    name          session  \\\n",
       "0                  NaN           Marianne Lake  question_answer   \n",
       "1                  0.0           Pierre Breber  question_answer   \n",
       "2                  NaN           James Taiclet  question_answer   \n",
       "3                  NaN         Daniel T. Henry  question_answer   \n",
       "4                  NaN      Cherilyn Radbourne  question_answer   \n",
       "...                ...                     ...              ...   \n",
       "2101               0.0             Scott Davis  question_answer   \n",
       "2102               NaN              Larry Fink  question_answer   \n",
       "2103              -1.0             Derica Rice  question_answer   \n",
       "2104               NaN  Patricia E. Yarrington  question_answer   \n",
       "2105               NaN          Hamid Moghadam  question_answer   \n",
       "\n",
       "                  time          id  quarter  ... Shimmer local  \\\n",
       "0     11/10/2013 16:36   JPM_57713        3  ...        12.130   \n",
       "1     29/01/2021 23:14  CVX_181025        4  ...        14.836   \n",
       "2     21/07/2020 21:21  LMT_171182        2  ...        12.822   \n",
       "3     23/10/2009 04:51   AXP_22181        3  ...        14.428   \n",
       "4     18/04/2019 21:18  UNP_149565        1  ...        17.217   \n",
       "...                ...         ...      ...  ...           ...   \n",
       "2101  28/07/2015 19:02   HON_86730        2  ...        21.467   \n",
       "2102  16/01/2015 04:09   BLK_77575        4  ...        21.663   \n",
       "2103  21/10/2010 20:35   LLY_28408        3  ...        19.008   \n",
       "2104  27/04/2012 19:51   CVX_37942        1  ...        12.630   \n",
       "2105  21/07/2020 21:03  PLD_171179        2  ...        13.721   \n",
       "\n",
       "      Shimmer local dB Shimmer apq3  Shimmer apq5  Shimmer apq11  Shimmer dda  \\\n",
       "0                1.138        4.807         7.172         12.167       14.422   \n",
       "1                1.244        6.570         9.223         13.112       19.710   \n",
       "2                1.228        5.390         7.708         13.904       16.169   \n",
       "3                1.260        6.723         9.102         12.561       20.169   \n",
       "4                1.483        7.715        10.271         18.200       23.144   \n",
       "...                ...          ...           ...            ...          ...   \n",
       "2101             1.755       10.289        16.527         11.007       30.866   \n",
       "2102             1.711       10.139        18.930         27.440       30.416   \n",
       "2103             1.620        9.075        12.276         18.522       27.224   \n",
       "2104             1.035        5.229         6.777          8.244       15.688   \n",
       "2105             1.269        6.435         9.067         12.255       19.306   \n",
       "\n",
       "      Mean autocorrelation  Mean NHR  Mean HNR  Audio Length  \n",
       "0                 0.862177  0.194742    10.197     17.513991  \n",
       "1                 0.786955  0.311092     6.546      7.341958  \n",
       "2                 0.834549  0.244605     8.601     40.501746  \n",
       "3                 0.840758  0.206720     7.759     24.655147  \n",
       "4                 0.751303  0.386504     5.624      5.481270  \n",
       "...                    ...       ...       ...           ...  \n",
       "2101              0.788477  0.300832     6.411     58.961875  \n",
       "2102              0.741760  0.402328     5.312      2.973515  \n",
       "2103              0.770155  0.347879     6.351      8.772698  \n",
       "2104              0.871881  0.174371    10.346     36.462494  \n",
       "2105              0.689297  0.484395     3.661      3.549958  \n",
       "\n",
       "[2106 rows x 43 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60558cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       " 1.0    702\n",
       " 0.0    702\n",
       "-1.0    702\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34422adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       And if you think about 11%plus tier one capita...\n",
       "1            So there is no primary need on balance sheet\n",
       "2       If theyre not sure theyre going to get paid fo...\n",
       "3       You may have seen that we were rated 1 in cust...\n",
       "4                                      That’s all from me\n",
       "                              ...                        \n",
       "2101                                What was the process?\n",
       "2102    But I do believe in this divergent world and t...\n",
       "2103    I wan to remind the investment community once ...\n",
       "2104    That was  the quarter recognized 2 of those dr...\n",
       "2105    On cap rate compression, we are already seeing...\n",
       "Name: speech, Length: 2106, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07856cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train and test sets using a stratified split to ensure the above proportions are kept\n",
    "#for both the training and test sets\n",
    "\n",
    "X = sampled_df['speech']\n",
    "y = sampled_df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.20,\n",
    "                                                random_state=9)\n",
    "\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "021e16eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060    1.0\n",
       "144     0.0\n",
       "1639   -1.0\n",
       "643     1.0\n",
       "369     1.0\n",
       "       ... \n",
       "718    -1.0\n",
       "757     1.0\n",
       "1777    1.0\n",
       "97      0.0\n",
       "797     1.0\n",
       "Name: sentiment, Length: 422, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac47ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f08adb7",
   "metadata": {},
   "source": [
    "## Encoding sentences using BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39ec86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModel.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_cls_embeddings(texts, batch_size=4):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "248f0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 421/421 [02:22<00:00,  2.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 106/106 [00:37<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#generate embeddings for Xtrain and Xtest\n",
    "X_train_embeddings = get_cls_embeddings(X_train.tolist(), batch_size=4)\n",
    "X_test_embeddings = get_cls_embeddings(X_test.tolist(), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "552ad211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#structing the data to apply to the model\n",
    "def making_label(st):\n",
    "    if st == 0:\n",
    "        return 0\n",
    "    elif st == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "#0 = neutral, 1 = positive, 2 = negative\n",
    "\n",
    "#0 = neutral, 1 = positive, 2 = negative\n",
    "labels_train = y_train.apply(making_label)\n",
    "labels_test = y_test.apply(making_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d46bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05cbe06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64b46d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = X_train_embeddings.shape[1]\n",
    "input_shape = (n_cols,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d77551e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xeb15154\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7019 - loss: 0.8282\n",
      "Epoch 2/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7791 - loss: 0.5507\n",
      "Epoch 3/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8254 - loss: 0.4494\n",
      "Epoch 4/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8521 - loss: 0.3966\n",
      "Epoch 5/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8866 - loss: 0.3293\n",
      "Epoch 6/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8931 - loss: 0.2885\n",
      "Epoch 7/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9287 - loss: 0.2311\n",
      "Epoch 8/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9471 - loss: 0.1930\n",
      "Epoch 9/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9650 - loss: 0.1571\n",
      "Epoch 10/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9709 - loss: 0.1301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a2c16968d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_text = Sequential()\n",
    "model_text.add(Dense(768, activation = 'relu', input_shape = input_shape))\n",
    "model_text.add(Dense(3, activation = 'softmax', kernel_initializer = 'he_uniform'))\n",
    "model_text.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model_text.fit(X_train_embeddings, y_train, epochs = 10, batch_size =100)\n",
    "# loss: 0.7134 - accuracy: 0.7994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdbbcb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "predictions = model_text.predict(X_test_embeddings)\n",
    "predictions = pd.DataFrame(predictions)\n",
    "y_pred = predictions.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "412486fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment  sentiment_predictions\n",
       "1060          1                      1\n",
       "144           0                      0\n",
       "1639          2                      2\n",
       "643           1                      1\n",
       "369           1                      1\n",
       "...         ...                    ...\n",
       "718           2                      2\n",
       "757           1                      1\n",
       "1777          1                      1\n",
       "97            0                      0\n",
       "797           1                      1\n",
       "\n",
       "[422 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df = pd.DataFrame(y_test)\n",
    "testing_df['sentiment'] = testing_df['sentiment'].astype(int)\n",
    "testing_df['sentiment'] = testing_df['sentiment'].apply(making_label)\n",
    "testing_df['sentiment_predictions'] = list(y_pred)\n",
    "testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d33038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(testing_df['sentiment'], testing_df['sentiment_predictions'])\n",
    "print(f'Accuracy: {accuracy:.4f}')  # prints accuracy as a decimal, e.g., 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44d86608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing working directory\n",
    "os.chdir('C:/Users/xeb15154/OneDrive - University of Strathclyde/Financial Sentiment Analysis Project/')\n",
    "\n",
    "#Saving the model\n",
    "\n",
    "with open(\"model_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d01e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd49ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
